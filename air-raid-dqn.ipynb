{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T02:02:43.392371Z",
     "start_time": "2025-09-25T02:02:38.507192Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "import numpy as np\n",
    "import cv2\n",
    "import gymnasium as gym\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import ale_py\n",
    "class AtariPreprocess:\n",
    "    def __init__(self, width=84, height=84):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "\n",
    "    def __call__(self, obs):\n",
    "        img = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)\n",
    "        img = cv2.resize(img, (self.width, self.height), interpolation=cv2.INTER_AREA)\n",
    "        return np.ascontiguousarray(img, dtype=np.uint8)\n",
    "\n",
    "class FrameStack:\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "        self.frames = deque(maxlen=k)\n",
    "\n",
    "    def reset(self, frame):\n",
    "        for _ in range(self.k):\n",
    "            self.frames.append(frame)\n",
    "        return self._get_obs()\n",
    "\n",
    "    def append(self, frame):\n",
    "        self.frames.append(frame)\n",
    "        return self._get_obs()\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return np.stack(self.frames, axis=0)\n",
    "\n",
    "class SkipEnvWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, skip=4):\n",
    "        super().__init__(env)\n",
    "        self.skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = False\n",
    "        for i in range(self.skip):\n",
    "            obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            done = terminated or truncated\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, terminated, truncated, info\n",
    "\n",
    "def make_env(env_id=\"ALE/air_raid-v5\", seed=0, skip=4, stack=4):\n",
    "    raw = gym.make(env_id, render_mode=None)\n",
    "    raw.reset(seed=seed)\n",
    "    pre = AtariPreprocess()\n",
    "    skipw = SkipEnvWrapper(raw, skip=skip)\n",
    "    stacker = FrameStack(stack)\n",
    "    class EnvObj:\n",
    "        def __init__(self, e, pre, stacker):\n",
    "            self.e = e\n",
    "            self.pre = pre\n",
    "            self.stacker = stacker\n",
    "            self.action_space = e.action_space\n",
    "            self.observation_space = gym.spaces.Box(\n",
    "                low=0, high=255, shape=(stacker.k, pre.height, pre.width), dtype=np.uint8\n",
    "            )\n",
    "\n",
    "        def reset(self):\n",
    "            obs, info = self.e.reset()\n",
    "            obs_proc = self.pre(obs)\n",
    "            stacked = self.stacker.reset(obs_proc)\n",
    "            return stacked, info\n",
    "\n",
    "        def step(self, action):\n",
    "            obs, reward, terminated, truncated, info = self.e.step(action)\n",
    "            obs_proc = self.pre(obs)\n",
    "            stacked = self.stacker.append(obs_proc)\n",
    "            reward = np.clip(reward, -1.0, 1.0)\n",
    "            return stacked, reward, terminated, truncated, info\n",
    "\n",
    "        def render(self, *args, **kwargs):\n",
    "            return self.e.render(*args, **kwargs)\n",
    "\n",
    "        def close(self):\n",
    "            return self.e.close()\n",
    "\n",
    "    return EnvObj(skipw, pre, stacker)\n",
    "\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity, device):\n",
    "        self.capacity = capacity\n",
    "        self.device = device\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.buffer.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states = torch.from_numpy(np.stack([b.state for b in batch])).float().to(self.device) / 255.0\n",
    "        actions = torch.tensor([b.action for b in batch], dtype=torch.long, device=self.device)\n",
    "        rewards = torch.tensor([b.reward for b in batch], dtype=torch.float32, device=self.device)\n",
    "        next_states = torch.from_numpy(np.stack([b.next_state for b in batch])).float().to(self.device) / 255.0\n",
    "        dones = torch.tensor([float(b.done) for b in batch], dtype=torch.float32, device=self.device)\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, in_channels, n_actions):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self._conv_out = self._get_conv_out(in_channels)\n",
    "        self.fc1 = nn.Linear(self._conv_out, 512)\n",
    "        self.fc2 = nn.Linear(512, n_actions)\n",
    "\n",
    "    def _get_conv_out(self, in_channels):\n",
    "        x = torch.zeros(1, in_channels, 84, 84)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        return int(np.prod(x.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, in_channels, n_actions, device, lr=1e-4, gamma=0.99,\n",
    "                 buffer_size=100000, batch_size=32, target_update=1000):\n",
    "        self.device = device\n",
    "        self.n_actions = n_actions\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update = target_update\n",
    "        self.policy_net = DQN(in_channels, n_actions).to(device)\n",
    "        self.target_net = DQN(in_channels, n_actions).to(device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "        self.replay = ReplayBuffer(buffer_size, device)\n",
    "        self.steps_done = 0\n",
    "\n",
    "    def select_action(self, state, epsilon):\n",
    "        if random.random() < epsilon:\n",
    "            return random.randrange(self.n_actions)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                s = torch.from_numpy(state).float().unsqueeze(0).to(self.device) / 255.0\n",
    "                q = self.policy_net(s)\n",
    "                return int(q.argmax(dim=1).item())\n",
    "\n",
    "    def optimize(self):\n",
    "        if len(self.replay) < self.batch_size:\n",
    "            return None\n",
    "        states, actions, rewards, next_states, dones = self.replay.sample(self.batch_size)\n",
    "        q_values = self.policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        with torch.no_grad():\n",
    "            #next_q = self.target_net(next_states).max(1)[0]\n",
    "            next_actions = self.policy_net(next_states).argmax(1)\n",
    "            next_q = self.target_net(next_states).gather(1, next_actions.unsqueeze(1)).squeeze(1)\n",
    "            target_q = rewards + (1.0 - dones) * self.gamma * next_q\n",
    "        loss = F.smooth_l1_loss(q_values, target_q)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.policy_net.parameters(), 10.0)\n",
    "        self.optimizer.step()\n",
    "        self.steps_done += 1\n",
    "        if self.steps_done % self.target_update == 0:\n",
    "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        return loss.item()\n",
    "\n",
    "    def save(self, path):\n",
    "        torch.save({\n",
    "            'policy_state_dict': self.policy_net.state_dict(),\n",
    "            'target_state_dict': self.target_net.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "        }, path)\n",
    "\n",
    "    def load(self, path):\n",
    "        ck = torch.load(path, map_location=self.device)\n",
    "        self.policy_net.load_state_dict(ck['policy_state_dict'])\n",
    "        self.target_net.load_state_dict(ck['target_state_dict'])\n",
    "        self.optimizer.load_state_dict(ck['optimizer_state_dict'])\n",
    "\n",
    "def train(env_id='ALE/air_raid-v5',\n",
    "          seed=42,\n",
    "          total_steps=2_000_000,\n",
    "          start_learning=50_000,\n",
    "          eval_interval=50_000,\n",
    "          save_path='dqn_airraid.pth'):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    env = make_env(env_id, seed=seed, skip=4, stack=4)\n",
    "    eval_env = make_env(env_id, seed=seed+1, skip=4, stack=4)\n",
    "    obs, _ = env.reset()\n",
    "    in_channels = obs.shape[0]\n",
    "    n_actions = env.action_space.n\n",
    "    agent = DQNAgent(in_channels, n_actions, device,\n",
    "                     lr=1e-4, gamma=0.99,\n",
    "                     buffer_size=500000, batch_size=64, target_update=1000)\n",
    "    epsilon_start = 1.0\n",
    "    epsilon_final = 0.1\n",
    "    epsilon_decay = 1_000_000\n",
    "    state, _ = env.reset()\n",
    "    episode_reward = 0.0\n",
    "    episode_cnt = 0\n",
    "    total_step = 0\n",
    "    losses = []\n",
    "\n",
    "    while total_step < total_steps:\n",
    "        eps = epsilon_final + (epsilon_start - epsilon_final) * max(0, (epsilon_decay - total_step) / epsilon_decay)\n",
    "        action = agent.select_action(state, eps)\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        agent.replay.push(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        total_step += 1\n",
    "        if len(agent.replay) >= start_learning:\n",
    "            loss = agent.optimize()\n",
    "            if loss is not None:\n",
    "                losses.append(loss)\n",
    "        if done:\n",
    "            state, _ = env.reset()\n",
    "            episode_cnt += 1\n",
    "            print(f\"Step {total_step} | Episode {episode_cnt} ended | Reward {episode_reward:.2f} | Eps {eps:.3f}\")\n",
    "            episode_reward = 0.0\n",
    "        if total_step % eval_interval == 0:\n",
    "            avg_score = evaluate(agent, eval_env, episodes=5)\n",
    "            print(f\"== Eval at step {total_step}: avg score = {avg_score:.2f} ==\")\n",
    "    agent.save(save_path)\n",
    "    env.close()\n",
    "    eval_env.close()\n",
    "\n",
    "def evaluate(agent:DQNAgent, env, episodes=5, render=False):\n",
    "    device = agent.device\n",
    "    scores = []\n",
    "    for ep in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        total = 0.0\n",
    "        while not done:\n",
    "            action = agent.select_action(state, epsilon=0.001)  # near-greedy\n",
    "            state, reward, terminated, truncated, info = env.step(action)\n",
    "            total += reward\n",
    "            done = terminated or truncated\n",
    "            if render:\n",
    "                env.render()\n",
    "        scores.append(total)\n",
    "    return float(np.mean(scores))\n",
    "\n",
    "'''\n",
    "if __name__ == \"__main__\":\n",
    "    train(env_id=\"ALE/AirRaid-v5\",\n",
    "          seed=123,\n",
    "          total_steps=2_000_000,\n",
    "          start_learning=50_000,\n",
    "          eval_interval=50_000,\n",
    "          save_path=\"dqn_airraid_final.pth\")\n",
    "\n",
    "'''"
   ],
   "id": "ac196bb0f9c5c443",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nif __name__ == \"__main__\":\\n    train(env_id=\"ALE/AirRaid-v5\",\\n          seed=123,\\n          total_steps=2_000_000,\\n          start_learning=50_000,\\n          eval_interval=50_000,\\n          save_path=\"dqn_airraid_final.pth\")\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T02:02:59.650330Z",
     "start_time": "2025-09-25T02:02:43.843095Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import numpy as np\n",
    "import ale_py\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "env = make_env(env_id=\"ALE/AirRaid-v5\", seed=42, skip=4, stack=4)\n",
    "env.e = gym.make(\"ALE/AirRaid-v5\", render_mode=\"human\")\n",
    "\n",
    "num_actions = env.action_space.n\n",
    "obs, _ = env.reset()\n",
    "in_channels = obs.shape[0]\n",
    "policy_net = DQN(in_channels,num_actions).to(device)\n",
    "checkpoint = torch.load(\"models/dqn_airraid_final.pth\", map_location=device)\n",
    "policy_net.load_state_dict(checkpoint['policy_state_dict'])  # 从子字典提取\n",
    "policy_net.eval()\n",
    "episodes = 3\n",
    "for ep in range(episodes):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    while not done:\n",
    "        s = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0) / 255.0\n",
    "        with torch.no_grad():\n",
    "            q_values = policy_net(s)\n",
    "            action = q_values.argmax(1).item()\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        env.render()\n",
    "    print(f\"Episode {ep+1} finished, score = {score}\")\n",
    "\n",
    "env.close()\n",
    "\n"
   ],
   "id": "e5b96ec37708cd1a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\18735\\AppData\\Local\\Temp\\ipykernel_16408\\1605448978.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"models/dqn_airraid_final.pth\", map_location=device)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 28\u001B[39m\n\u001B[32m     26\u001B[39m     q_values = policy_net(s)\n\u001B[32m     27\u001B[39m     action = q_values.argmax(\u001B[32m1\u001B[39m).item()\n\u001B[32m---> \u001B[39m\u001B[32m28\u001B[39m next_state, reward, terminated, truncated, _ = \u001B[43menv\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     29\u001B[39m done = terminated \u001B[38;5;129;01mor\u001B[39;00m truncated\n\u001B[32m     30\u001B[39m state = next_state\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 79\u001B[39m, in \u001B[36mmake_env.<locals>.EnvObj.step\u001B[39m\u001B[34m(self, action)\u001B[39m\n\u001B[32m     78\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mstep\u001B[39m(\u001B[38;5;28mself\u001B[39m, action):\n\u001B[32m---> \u001B[39m\u001B[32m79\u001B[39m     obs, reward, terminated, truncated, info = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43me\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     80\u001B[39m     obs_proc = \u001B[38;5;28mself\u001B[39m.pre(obs)\n\u001B[32m     81\u001B[39m     stacked = \u001B[38;5;28mself\u001B[39m.stacker.append(obs_proc)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\conda\\conda\\envs\\zxh\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:393\u001B[39m, in \u001B[36mOrderEnforcing.step\u001B[39m\u001B[34m(self, action)\u001B[39m\n\u001B[32m    391\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m._has_reset:\n\u001B[32m    392\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m ResetNeeded(\u001B[33m\"\u001B[39m\u001B[33mCannot call env.step() before calling env.reset()\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m393\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\conda\\conda\\envs\\zxh\\Lib\\site-packages\\gymnasium\\core.py:327\u001B[39m, in \u001B[36mWrapper.step\u001B[39m\u001B[34m(self, action)\u001B[39m\n\u001B[32m    323\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mstep\u001B[39m(\n\u001B[32m    324\u001B[39m     \u001B[38;5;28mself\u001B[39m, action: WrapperActType\n\u001B[32m    325\u001B[39m ) -> \u001B[38;5;28mtuple\u001B[39m[WrapperObsType, SupportsFloat, \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any]]:\n\u001B[32m    326\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m327\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43menv\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\conda\\conda\\envs\\zxh\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:285\u001B[39m, in \u001B[36mPassiveEnvChecker.step\u001B[39m\u001B[34m(self, action)\u001B[39m\n\u001B[32m    283\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m env_step_passive_checker(\u001B[38;5;28mself\u001B[39m.env, action)\n\u001B[32m    284\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m285\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43menv\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\conda\\conda\\envs\\zxh\\Lib\\site-packages\\ale_py\\env.py:250\u001B[39m, in \u001B[36mAtariEnv.step\u001B[39m\u001B[34m(self, action)\u001B[39m\n\u001B[32m    248\u001B[39m reward = \u001B[32m0.0\u001B[39m\n\u001B[32m    249\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(frameskip):\n\u001B[32m--> \u001B[39m\u001B[32m250\u001B[39m     reward += \u001B[38;5;28mself\u001B[39m.ale.act(\u001B[38;5;28mself\u001B[39m._action_set[action])\n\u001B[32m    251\u001B[39m is_terminal = \u001B[38;5;28mself\u001B[39m.ale.game_over(with_truncation=\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[32m    252\u001B[39m is_truncated = \u001B[38;5;28mself\u001B[39m.ale.game_truncated()\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
