## 输入的训练数据的格式
输入序列的格式是：对于语料库中的一个序列 $ x_{0}...x_{T} $

在每一个位置 $ t $，前面的部分 $ x_{<t} $ 被视为上下文 (context)

而 $ x_{t} $ 则是需要预测的真实下一个词元 (ground-truth next token)

## 模型奖励的定义
假设 $ x_{\ge t} $ 是语料库中真实的后续文本序列，$ y_{t}^{i} $ 是模型生成的第 $ i $ 个预测。将这两个序列转换为字节序列，分别表示为 $ \overline{x}_{\ge t} $ 和 $ \overline{y}_{t}^{i} $。
奖励 $r_{t}^{i}$ 的计算方式为：
- **奖励为 1**：当且仅当预测的字节序列 $ \overline{y}_{t}^{i} $ 是真实后续文本字节序列 $ \overline{x}_{\ge t} $ 的一个精确前缀，并且其字节长度 $ l $与真实序列中某个词元的边界完全吻合时 。
- **奖励为 0**：在其他所有情况下 。

## 用于RL pretrain的方法
- On-policy的策略优化
    - 保证采样分布与当前策略一致，避免分布偏移和高方差；对于“生成推理链 → 最终预测”的任务，必须保证奖励与当前策略行为匹配，可以使收敛比较稳定。
- GRPO算法
    - 相比于PPO减去了Value model
    - 提供更稳定的策略更新，相比直接策略梯度降低方差，在大模型、稀疏奖励场景下提高训练稳定性和收敛速度。
- 前缀匹配奖励
    - 奖励可以被直接验证，不需要需人工的偏好模型；能覆盖多token和 out-of-vocabulary情况，一定程度上避免reward hacking；提供清晰的正确/错误信号，适用于大规模无标注文本。
