{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-25T01:21:36.771742Z",
     "start_time": "2025-09-25T01:21:31.080808Z"
    }
   },
   "source": [
    "import os\n",
    "from collections import deque, namedtuple\n",
    "import numpy as np\n",
    "import cv2\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import ale_py\n",
    "\n",
    "class AtariPreprocess:\n",
    "    def __init__(self, width=84, height=84):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "\n",
    "    def __call__(self, obs):\n",
    "        img = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)\n",
    "        img = cv2.resize(img, (self.width, self.height), interpolation=cv2.INTER_AREA)\n",
    "        return np.ascontiguousarray(img, dtype=np.uint8)\n",
    "\n",
    "class FrameStack:\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "        self.frames = deque(maxlen=k)\n",
    "\n",
    "    def reset(self, frame):\n",
    "        for _ in range(self.k):\n",
    "            self.frames.append(frame)\n",
    "        return self._get_obs()\n",
    "\n",
    "    def append(self, frame):\n",
    "        self.frames.append(frame)\n",
    "        return self._get_obs()\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return np.stack(self.frames, axis=0)\n",
    "\n",
    "class SkipEnvWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, skip=4):\n",
    "        super().__init__(env)\n",
    "        self.skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = False\n",
    "        for i in range(self.skip):\n",
    "            obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            done = terminated or truncated\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, terminated, truncated, info\n",
    "\n",
    "def make_env(env_id=\"ALE/AirRaid-v5\", seed=0, skip=4, stack=4):\n",
    "    raw = gym.make(env_id, render_mode=None)\n",
    "    raw.reset(seed=seed)\n",
    "    pre = AtariPreprocess()\n",
    "    skipw = SkipEnvWrapper(raw, skip=skip)\n",
    "    stacker = FrameStack(stack)\n",
    "\n",
    "    class EnvObj:\n",
    "        def __init__(self, e, pre, stacker):\n",
    "            self.e = e\n",
    "            self.pre = pre\n",
    "            self.stacker = stacker\n",
    "            self.action_space = e.action_space\n",
    "            self.observation_space = gym.spaces.Box(\n",
    "                low=0, high=255, shape=(stacker.k, pre.height, pre.width), dtype=np.uint8\n",
    "            )\n",
    "\n",
    "        def reset(self):\n",
    "            obs, info = self.e.reset()\n",
    "            obs_proc = self.pre(obs)\n",
    "            stacked = self.stacker.reset(obs_proc)\n",
    "            return stacked, info\n",
    "\n",
    "        def step(self, action):\n",
    "            obs, reward, terminated, truncated, info = self.e.step(action)\n",
    "            obs_proc = self.pre(obs)\n",
    "            stacked = self.stacker.append(obs_proc)\n",
    "            reward = np.clip(reward, -1.0, 1.0)\n",
    "            return stacked, reward, terminated, truncated, info\n",
    "\n",
    "        def render(self, *args, **kwargs):\n",
    "            return self.e.render(*args, **kwargs)\n",
    "\n",
    "        def close(self):\n",
    "            return self.e.close()\n",
    "\n",
    "    return EnvObj(skipw, pre, stacker)\n",
    "\n",
    "Experience = namedtuple('Experience', ('state', 'action', 'reward', 'next_state', 'done', 'log_prob', 'value'))\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, in_channels, n_actions):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self._conv_out = self._get_conv_out(in_channels)\n",
    "        self.fc_shared = nn.Linear(self._conv_out, 512)\n",
    "        self.actor = nn.Linear(512, n_actions)\n",
    "        self.critic = nn.Linear(512, 1)\n",
    "\n",
    "    def _get_conv_out(self, in_channels):\n",
    "        x = torch.zeros(1, in_channels, 84, 84)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        return int(np.prod(x.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc_shared(x))\n",
    "        action_probs = F.softmax(self.actor(x), dim=-1)\n",
    "        state_value = self.critic(x)\n",
    "        return action_probs, state_value\n",
    "\n",
    "class PPOBuffer:\n",
    "    def __init__(self, capacity, device):\n",
    "        self.capacity = capacity\n",
    "        self.device = device\n",
    "        self.buffer = []\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.buffer.append(Experience(*args))\n",
    "\n",
    "    def get_all(self):\n",
    "        if len(self.buffer) == 0:\n",
    "            return None\n",
    "        states = torch.from_numpy(np.stack([e.state for e in self.buffer])).float().to(self.device) / 255.0\n",
    "        actions = torch.tensor([e.action for e in self.buffer], dtype=torch.long, device=self.device)\n",
    "        rewards = torch.tensor([e.reward for e in self.buffer], dtype=torch.float32, device=self.device)\n",
    "        next_states = torch.from_numpy(np.stack([e.next_state for e in self.buffer])).float().to(self.device) / 255.0\n",
    "        dones = torch.tensor([float(e.done) for e in self.buffer], dtype=torch.float32, device=self.device)\n",
    "        log_probs = torch.tensor([e.log_prob for e in self.buffer], dtype=torch.float32, device=self.device)\n",
    "        values = torch.tensor([e.value for e in self.buffer], dtype=torch.float32, device=self.device)\n",
    "        return states, actions, rewards, next_states, dones, log_probs, values\n",
    "\n",
    "    def clear(self):\n",
    "        self.buffer = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(self, in_channels, n_actions, device, lr=2.5e-4, gamma=0.99,\n",
    "                 gae_lambda=0.95, clip_epsilon=0.2, entropy_coeff=0.01, value_coeff=0.5):\n",
    "        self.device = device\n",
    "        self.n_actions = n_actions\n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.clip_epsilon = clip_epsilon\n",
    "        self.entropy_coeff = entropy_coeff\n",
    "        self.value_coeff = value_coeff\n",
    "        self.actor_critic = ActorCritic(in_channels, n_actions).to(device)\n",
    "        self.optimizer = optim.Adam(self.actor_critic.parameters(), lr=lr, eps=1e-5)\n",
    "        self.buffer = PPOBuffer(50000, device)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(self.device) / 255.0\n",
    "        with torch.no_grad():\n",
    "            action_probs, value = self.actor_critic(state_tensor)\n",
    "        dist = Categorical(action_probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action).item()\n",
    "        return action.item(), log_prob, value.item()\n",
    "\n",
    "    def compute_gae(self, rewards, values, next_values, dones):\n",
    "        advantages = torch.zeros_like(rewards)\n",
    "        gae = 0\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            if t == len(rewards) - 1:\n",
    "                next_value = next_values[-1] if not dones[t] else 0\n",
    "            else:\n",
    "                next_value = values[t + 1]\n",
    "            delta = rewards[t] + self.gamma * next_value * (1 - dones[t]) - values[t]\n",
    "            gae = delta + self.gamma * self.gae_lambda * (1 - dones[t]) * gae\n",
    "            advantages[t] = gae\n",
    "        returns = advantages + values\n",
    "        return advantages, returns\n",
    "\n",
    "    def update(self, ppo_epochs=4, mini_batch_size=256):\n",
    "        if len(self.buffer) == 0:\n",
    "            return None, None, None\n",
    "        states, actions, rewards, next_states, dones, old_log_probs, values = self.buffer.get_all()\n",
    "        with torch.no_grad():\n",
    "            _, next_values = self.actor_critic(next_states)\n",
    "            next_values = next_values.squeeze()\n",
    "        advantages, returns = self.compute_gae(rewards, values, next_values, dones)\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        old_log_probs = torch.tensor(old_log_probs, device=self.device)\n",
    "        total_policy_loss = 0\n",
    "        total_value_loss = 0\n",
    "        total_entropy_loss = 0\n",
    "        for _ in range(ppo_epochs):\n",
    "            batch_size = len(states)\n",
    "            indices = torch.randperm(batch_size)\n",
    "            for start in range(0, batch_size, mini_batch_size):\n",
    "                end = start + mini_batch_size\n",
    "                batch_indices = indices[start:end]\n",
    "                batch_states = states[batch_indices]\n",
    "                batch_actions = actions[batch_indices]\n",
    "                batch_old_log_probs = old_log_probs[batch_indices]\n",
    "                batch_advantages = advantages[batch_indices]\n",
    "                batch_returns = returns[batch_indices]\n",
    "                action_probs, state_values = self.actor_critic(batch_states)\n",
    "                dist = Categorical(action_probs)\n",
    "                new_log_probs = dist.log_prob(batch_actions)\n",
    "                entropy = dist.entropy().mean()\n",
    "                ratio = torch.exp(new_log_probs - batch_old_log_probs)\n",
    "                surr1 = ratio * batch_advantages\n",
    "                surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * batch_advantages\n",
    "                policy_loss = -torch.min(surr1, surr2).mean()\n",
    "                state_values = state_values.squeeze()\n",
    "                value_loss = F.mse_loss(state_values, batch_returns)\n",
    "                total_loss = policy_loss + self.value_coeff * value_loss - self.entropy_coeff * entropy\n",
    "                self.optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.actor_critic.parameters(), 0.5)\n",
    "                self.optimizer.step()\n",
    "                total_policy_loss += policy_loss.item()\n",
    "                total_value_loss += value_loss.item()\n",
    "                total_entropy_loss += entropy.item()\n",
    "        num_updates = ppo_epochs * (batch_size // mini_batch_size + 1)\n",
    "        return (total_policy_loss / num_updates,\n",
    "                total_value_loss / num_updates,\n",
    "                total_entropy_loss / num_updates)\n",
    "\n",
    "    def save(self, path):\n",
    "        torch.save({\n",
    "            'actor_critic_state_dict': self.actor_critic.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "        }, path)\n",
    "\n",
    "    def load(self, path):\n",
    "        checkpoint = torch.load(path, map_location=self.device)\n",
    "        self.actor_critic.load_state_dict(checkpoint['actor_critic_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "def train(env_id='ALE/AirRaid-v5',\n",
    "          seed=42,\n",
    "          total_steps=2_000_000,\n",
    "          update_interval=2048,\n",
    "          eval_interval=50_000,\n",
    "          save_path='ppo_airraid.pth'):\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    env = make_env(env_id, seed=seed, skip=4, stack=4)\n",
    "    eval_env = make_env(env_id, seed=seed+1, skip=4, stack=4)\n",
    "    obs, _ = env.reset()\n",
    "    in_channels = obs.shape[0]\n",
    "    n_actions = env.action_space.n\n",
    "    agent = PPOAgent(in_channels, n_actions, device,\n",
    "                    lr=2.5e-4, gamma=0.99, gae_lambda=0.95,\n",
    "                    clip_epsilon=0.2, entropy_coeff=0.01, value_coeff=0.5)\n",
    "    state, _ = env.reset()\n",
    "    episode_reward = 0.0\n",
    "    episode_cnt = 0\n",
    "    total_step = 0\n",
    "    policy_losses = []\n",
    "    value_losses = []\n",
    "    entropy_losses = []\n",
    "    print(f\"Starting PPO training on {device}\")\n",
    "    while total_step < total_steps:\n",
    "        action, log_prob, value = agent.select_action(state)\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        agent.buffer.push(state, action, reward, next_state, done, log_prob, value)\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        total_step += 1\n",
    "        if done:\n",
    "            state, _ = env.reset()\n",
    "            episode_cnt += 1\n",
    "            print(f\"Step {total_step} | Episode {episode_cnt} ended | Reward {episode_reward:.2f}\")\n",
    "            episode_reward = 0.0\n",
    "        if len(agent.buffer) >= update_interval:\n",
    "            policy_loss, value_loss, entropy_loss = agent.update(ppo_epochs=4, mini_batch_size=256)\n",
    "            if policy_loss is not None:\n",
    "                policy_losses.append(policy_loss)\n",
    "                value_losses.append(value_loss)\n",
    "                entropy_losses.append(entropy_loss)\n",
    "                print(f\"Step {total_step} | Policy Loss: {policy_loss:.4f} | Value Loss: {value_loss:.4f} | Entropy: {entropy_loss:.4f}\")\n",
    "            agent.buffer.clear()\n",
    "        if total_step % eval_interval == 0 and total_step > 0:\n",
    "            avg_score = evaluate(agent, eval_env, episodes=5)\n",
    "            print(f\"== Eval at step {total_step}: avg score = {avg_score:.2f} ==\")\n",
    "    agent.save(save_path)\n",
    "    env.close()\n",
    "    eval_env.close()\n",
    "    print(f\"Training completed! Model saved to {save_path}\")\n",
    "\n",
    "def evaluate(agent: PPOAgent, env, episodes=5, render=False):\n",
    "    scores = []\n",
    "    for ep in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        total = 0.0\n",
    "        while not done:\n",
    "            action, _, _ = agent.select_action(state)\n",
    "            state, reward, terminated, truncated, info = env.step(action)\n",
    "            total += reward\n",
    "            done = terminated or truncated\n",
    "            if render:\n",
    "                env.render()\n",
    "        scores.append(total)\n",
    "    return float(np.mean(scores))\n",
    "\n",
    "'''\n",
    "if __name__ == \"__main__\":\n",
    "    # Train the model\n",
    "    train(env_id=\"ALE/AirRaid-v5\",\n",
    "          seed=42,\n",
    "          total_steps=2_000_000,\n",
    "          update_interval=2048,\n",
    "          eval_interval=50_000,\n",
    "          save_path=\"ppo_airraid.pth\")\n",
    "'''\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nif __name__ == \"__main__\":\\n    # Train the model\\n    train(env_id=\"ALE/AirRaid-v5\",\\n          seed=42,\\n          total_steps=2_000_000,\\n          update_interval=2048,\\n          eval_interval=50_000,\\n          save_path=\"ppo_airraid.pth\")\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T01:27:15.977540Z",
     "start_time": "2025-09-25T01:21:39.933468Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def play_trained_model(model_path, env_id='ALE/AirRaid-v5', episodes=3):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    raw_env = gym.make(env_id, render_mode=\"human\")\n",
    "    env = make_env(env_id, seed=42, skip=4, stack=4)\n",
    "    env.e = raw_env\n",
    "    obs, _ = env.reset()\n",
    "    in_channels = obs.shape[0]\n",
    "    n_actions = env.action_space.n\n",
    "    agent = PPOAgent(in_channels, n_actions, device)\n",
    "    agent.load(model_path)\n",
    "    for ep in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "        step_count = 0\n",
    "        while not done:\n",
    "            action, _, _ = agent.select_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            step_count += 1\n",
    "            env.render()\n",
    "        print(f\"Episode {ep+1} finished | Score: {score:.2f} | Steps: {step_count}\")\n",
    "    env.close()\n",
    "play_trained_model(\"models/ppo_airraid.pth\")"
   ],
   "id": "4a0dd07a5ebc30a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\18735\\AppData\\Local\\Temp\\ipykernel_23316\\3685316122.py:292: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(path, map_location=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 finished | Score: 51.00 | Steps: 2525\n",
      "Episode 2 finished | Score: 65.00 | Steps: 2765\n",
      "Episode 3 finished | Score: 41.00 | Steps: 1731\n"
     ]
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
